# -*- coding: utf-8 -*-
"""Iris classification using deep learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AYwsV8YbXrTyYRfFpcAZZOJsZL2WcoW
"""

from google.colab import drive
drive.mount('/content/drive')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from time import perf_counter
import os
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Defining batch specifications
batch_size = 10
img_height = 224
img_width = 224

# Loading training data
training_ds = tf.keras.preprocessing.image_dataset_from_directory(
    '/content/drive/MyDrive/Deep learning dataset/dataset 1',xz
    validation_split=0.25,
    subset="training",
    seed=42,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=True  # Set to True for better randomization
)

# Loading testing data
validation_ds = tf.keras.preprocessing.image_dataset_from_directory(
    '/content/drive/MyDrive/Deep learning dataset/dataset 1',
    validation_split=0.25,
    subset="validation",
    seed=42,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.models import Model
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
# Preprocessing layers for VGG16
preprocess_input = tf.keras.applications.vgg16.preprocess_input

# Creating the VGG16 base model
base_model = VGG16(input_shape=(img_height, img_width, 3),
                   include_top=False,
                   weights='imagenet')

# Freezing the pre-trained layers
base_model.trainable = False

# Adding a custom classifier on top
inputs = tf.keras.Input(shape=(img_height, img_width, 3))
x = preprocess_input(inputs)
x = base_model(x, training=False)
x = Flatten()(x)  # Flatten the output before connecting to fully connected layers
x = Dense(256, activation='relu')(x)  # Add a dense layer for additional features
outputs = Dense(3, activation='softmax')(x)

# Creating the final model
vgg16_model = Model(inputs, outputs)

# Compiling the model
vgg16_model.compile(optimizer='adam',
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy'])

# Training the model
history = vgg16_model.fit(training_ds, validation_data=validation_ds, epochs=25)

import tensorflow as tf
from tensorflow.keras.applications import VGG19
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten
from tensorflow.keras.models import Model
from sklearn.metrics import classification_report, confusion_matrix

# Assuming img_height and img_width are predefined
img_height, img_width = 224, 224

# Preprocessing layers for VGG19
preprocess_input = tf.keras.applications.vgg19.preprocess_input

# Creating the VGG19 base model
base_model = VGG19(input_shape=(img_height, img_width, 3),
                   include_top=False,
                   weights='imagenet')

# Freezing the pre-trained layers
base_model.trainable = False

# Adding a custom classifier on top
inputs = tf.keras.Input(shape=(img_height, img_width, 3))
x = preprocess_input(inputs)
x = base_model(x, training=False)
x = Flatten()(x)  # Flatten the output before connecting to fully connected layers
x = Dense(256, activation='relu')(x)  # Add a dense layer for additional features
outputs = Dense(3, activation='softmax')(x)

# Creating the final model
vgg19_model = Model(inputs, outputs)

# Compiling the model
vgg19_model.compile(optimizer='adam',
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy'])

# Training the model
history = vgg19_model.fit(training_ds, validation_data=validation_ds, epochs=20)

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.models import Model
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Preprocessing layers for ResNet50
preprocess_input = tf.keras.applications.resnet50.preprocess_input
# Creating the ResNet50 base model
base_model = ResNet50(input_shape=(img_height, img_width, 3),
                      include_top=False,
                      weights='imagenet')

# Freezing the pre-trained layers
base_model.trainable = False

# Adding a custom classifier on top
inputs = tf.keras.Input(shape=(img_height, img_width, 3))
x = preprocess_input(inputs)
x = base_model(x, training=False)
x = GlobalAveragePooling2D()(x)  # Use GlobalAveragePooling2D instead of Flatten
x = Dense(256, activation='relu')(x)
outputs = Dense(3, activation='softmax')(x)

# Creating the final model
resnet50_model = Model(inputs, outputs)

# Compiling the model
resnet50_model.compile(optimizer='adam',
                       loss='sparse_categorical_crossentropy',
                       metrics=['accuracy'])

# Training the model
history = resnet50_model.fit(training_ds, validation_data=validation_ds, epochs=25)

